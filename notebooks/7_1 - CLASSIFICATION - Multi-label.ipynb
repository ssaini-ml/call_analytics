{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, make_scorer, multilabel_confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from src.settings import PATH_LABELED, PATH_OPENAI_EMBEDDINGS, RANDOM_STATE\n",
        "from src.hierarchical_evaluation import evaluation_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7_3_1 - Classification - Multi-label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEPARTMENT = \"CS\" # \"CS\" or \"PS\"\n",
        "FEATURE = \"topics\" # \"transcription\", \"summary\", \"topics\", \"transcription_customer_only\", \"summary_customer_only\"\n",
        "\n",
        "if DEPARTMENT == \"CS\":\n",
        "    FILE_PREFIX = \"20250113_20250212_CS_\"\n",
        "elif DEPARTMENT == \"PS\":\n",
        "    FILE_PREFIX = \"20250101_20250224_PS_\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_labeled_train = pd.read_csv(f\"{PATH_LABELED}/labeled_train_{DEPARTMENT}.csv\")\n",
        "df_labeled_test = pd.read_csv(f\"{PATH_LABELED}/labeled_test_{DEPARTMENT}.csv\")\n",
        "if FEATURE in [\"transcription\", \"summary\", \"topics\"]:\n",
        "    df_embeddings = pd.read_csv(f\"{PATH_OPENAI_EMBEDDINGS}/{FILE_PREFIX}{FEATURE}_embeddings_text-embedding-3-large.csv\")\n",
        "else:\n",
        "    df_embeddings = pd.read_csv(f\"{PATH_OPENAI_EMBEDDINGS}/{FILE_PREFIX}addedfeatures_{FEATURE}_embeddings_text-embedding-3-large.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Number of rows in embeddings: {df_labeled_train.shape[0]}\")\n",
        "\n",
        "# Since something could have gone wrong in preprocessing, we will only keep the embeddings that are in the labeled data\n",
        "df_labeled_train = df_labeled_train.merge(df_embeddings, on=\"id\", how=\"inner\")\n",
        "\n",
        "print(f\"Number of rows in embeddings after dropping unlabeled IDs: {df_labeled_train.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Number of rows in embeddings: {df_labeled_test.shape[0]}\")\n",
        "\n",
        "# Since something could have gone wrong in preprocessing, we will only keep the embeddings that are in the labeled data\n",
        "df_labeled_test = df_labeled_test.merge(df_embeddings, on=\"id\", how=\"inner\")\n",
        "\n",
        "print(f\"Number of rows in embeddings after dropping unlabeled IDs: {df_labeled_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling\n",
        "\n",
        "### Hierarchy Level 1\n",
        "We will focus on this first level of the hierarchy to start with. We're taking it on as a multi-label problem. These are the steps we will take:\n",
        "1. Create targets that include only unique level 1 labels\n",
        "2. Binarize targets\n",
        "3. Define models and parameter grids\n",
        "4. Define k-fold cross validation (can't use stratified approach with multi-label problem)\n",
        "5. Define the scorer\n",
        "6. Store best models\n",
        "7. Evaluate best models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's first create a column for level_1 labels only. We keep unique ones only.\n",
        "df_labeled_train[\"level_1\"] = df_labeled_train[\"label\"].apply(lambda x: list(set([x.split(\"/\")[0] for x in x.split(\",\")])))\n",
        "df_labeled_test[\"level_1\"] = df_labeled_test[\"label\"].apply(lambda x: list(set([x.split(\"/\")[0] for x in x.split(\",\")])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = df_labeled_train[\"level_1\"]\n",
        "X_train = df_labeled_train.drop(columns=[\"id\", \"usage\", \"label\", \"level_1\"])\n",
        "\n",
        "y_test = df_labeled_test[\"level_1\"]\n",
        "X_test = df_labeled_test.drop(columns=[\"id\", \"usage\", \"label\", \"level_1\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "y_bin_train = mlb.fit_transform(y_train)\n",
        "print(f\"Level 1 labels for case 0: {df_labeled_train.at[0, 'level_1']}\")\n",
        "print(f\"Level 1 labels in MultiLAbelBinarizer: {mlb.classes_}\")\n",
        "print(f\"Level 1 binarized labels for case 0: {y_bin_train[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    'SVC': {\n",
        "        'model': OneVsRestClassifier(SVC(probability=True)),  # SVC needs probability=True for OVR\n",
        "        'params': {\n",
        "            'estimator__C': [0.1, 1],  # Regularization strength\n",
        "            'estimator__kernel': ['linear'],  # Different decision boundaries\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define K-Fold for multilabel classification\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define scoring function (F1 score is often better for multilabel problems)\n",
        "scorer = make_scorer(f1_score, average='micro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_models = {}\n",
        "\n",
        "# Iterate over models and perform Grid Search\n",
        "for model_name, model_info in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    \n",
        "    grid_search = GridSearchCV(\n",
        "        model_info['model'], \n",
        "        model_info['params'], \n",
        "        cv=cv, \n",
        "        scoring=scorer, \n",
        "        n_jobs=-1, \n",
        "        verbose=2\n",
        "    )\n",
        "    \n",
        "    grid_search.fit(X_train, y_bin_train)\n",
        "    \n",
        "    # Store the best model for each classifier\n",
        "    best_models[model_name] = grid_search.best_estimator_\n",
        "    \n",
        "    # Print the best hyperparameters\n",
        "    print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_train = best_models['SVC'].predict(X_train)\n",
        "\n",
        "# for num, label in enumerate(mlb.classes_):\n",
        "#     cm = multilabel_confusion_matrix(y_bin_test, y_pred)\n",
        "#     print(f\"Label: {label}\")\n",
        "#     print(f\"Confusion Matrix: {cm}\")\n",
        "#     print(\"\\n\")\n",
        "print(classification_report(y_bin_train, y_pred_train, target_names=mlb.classes_, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_bin_test = mlb.transform(y_test)\n",
        "y_pred_test = best_models['SVC'].predict(X_test)\n",
        "\n",
        "# for num, label in enumerate(mlb.classes_):\n",
        "#     cm = multilabel_confusion_matrix(y_bin_test, y_pred)\n",
        "#     print(f\"Label: {label}\")\n",
        "#     print(f\"Confusion Matrix: {cm}\")\n",
        "#     print(\"\\n\")\n",
        "print(classification_report(y_bin_test, y_pred_test, target_names=mlb.classes_, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full hierarchy - Flat\n",
        "\n",
        "Here we'll do flat classification for all labels in the complete hierarchy. We use the same steps as specified above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_labeled_train[\"full_label\"] = df_labeled_train[\"label\"].apply(lambda x: x.split(\",\"))\n",
        "df_labeled_test[\"full_label\"] = df_labeled_test[\"label\"].apply(lambda x: x.split(\",\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = df_labeled_train[\"full_label\"]\n",
        "X_train = df_labeled_train.drop(columns=[\"id\", \"usage\", \"label\", \"level_1\", \"full_label\"])\n",
        "\n",
        "y_test = df_labeled_test[\"full_label\"]\n",
        "X_test = df_labeled_test.drop(columns=[\"id\", \"usage\", \"label\", \"level_1\", \"full_label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(y_train)\n",
        "y_bin_train = mlb.transform(y_train)\n",
        "y_bin_test = mlb.transform(y_test)\n",
        "print(f\"Labels for case 0: {df_labeled_train.at[0, 'full_label']}\")\n",
        "print(f\"Labels in MultiLAbelBinarizer: {mlb.classes_}\")\n",
        "print(f\"Binarized labels for case 0: {y_bin_train[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    'SVC': {\n",
        "        'model': OneVsRestClassifier(SVC(probability=True)),  # SVC needs probability=True for OVR\n",
        "        'params': {\n",
        "            'estimator__C': [1],  # Regularization strength\n",
        "            'estimator__kernel': ['rbf'],  # Different decision boundaries\n",
        "            'estimator__gamma': ['scale'],  # Kernel coefficient\n",
        "            'estimator__class_weight': ['balanced']   # Class weights\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define K-Fold for multilabel classification\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define scoring function (F1 score is often better for multilabel problems)\n",
        "scorer = make_scorer(f1_score, average='micro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_models = {}\n",
        "\n",
        "# Iterate over models and perform Grid Search\n",
        "for model_name, model_info in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    \n",
        "    grid_search = GridSearchCV(\n",
        "        model_info['model'], \n",
        "        model_info['params'], \n",
        "        cv=cv, \n",
        "        scoring=scorer, \n",
        "        n_jobs=-1, \n",
        "        verbose=2\n",
        "    )\n",
        "    \n",
        "    grid_search.fit(X_train, y_bin_train)\n",
        "    \n",
        "    # Store the best model for each classifier\n",
        "    best_models[model_name] = grid_search.best_estimator_\n",
        "    \n",
        "    # Print the best hyperparameters\n",
        "    print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_train = best_models['SVC'].predict(X_train)\n",
        "\n",
        "# for num, label in enumerate(mlb.classes_):\n",
        "#     cm = multilabel_confusion_matrix(y_bin_test, y_pred)\n",
        "#     print(f\"Label: {label}\")\n",
        "#     print(f\"Confusion Matrix: {cm}\")\n",
        "#     print(\"\\n\")\n",
        "print(classification_report(y_bin_train, y_pred_train, target_names=mlb.classes_, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_test = best_models['SVC'].predict(X_test)\n",
        "\n",
        "# for num, label in enumerate(mlb.classes_):\n",
        "#     cm = multilabel_confusion_matrix(y_bin_test, y_pred)\n",
        "#     print(f\"Label: {label}\")\n",
        "#     print(f\"Confusion Matrix: {cm}\")\n",
        "#     print(\"\\n\")\n",
        "print(classification_report(y_bin_test, y_pred_test, target_names=mlb.classes_, zero_division=0))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Embeddings (Python 3.12)",
      "language": "python",
      "name": "embeddings_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
